{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from scipy.io import wavfile\n",
    "import glob\n",
    "from nnAudio import Spectrogram\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "import time\n",
    "import tqdm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "p = os.path.join('../../../../pytorch_musicnet/data/', 'train_data', '*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali.pipeline import Pipeline\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "import nvidia.dali.ops as ops\n",
    "import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicNet(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "#         print(Path(__file__).parent)\n",
    "        self.file_list = glob.glob(p)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)  \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        audio_name = self.file_list[idx]\n",
    "        sr, wav = wavfile.read(audio_name)\n",
    "\n",
    "        return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MusicNet()\n",
    "dataset = DataLoader(dataset, shuffle=False, num_workers=8)\n",
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramPipeline(Pipeline):\n",
    "    def __init__(self, device, batch_size, num_threads=1, device_id=0):\n",
    "        super(SpectrogramPipeline, self).__init__(batch_size, num_threads, device_id)\n",
    "        self.device = device\n",
    "        self.batch_data = []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            y, sr = librosa.load(librosa.util.example_audio_file())\n",
    "            self.batch_data.append(np.array(y, dtype=np.float32))\n",
    "\n",
    "        self.external_source = ops.ExternalSource()\n",
    "        self.spectrogram = ops.Spectrogram(device=self.device,\n",
    "                                           nfft=nfft,\n",
    "                                           window_length=window_length,\n",
    "                                           window_step=window_step)\n",
    "\n",
    "    def define_graph(self):\n",
    "        self.data = self.external_source()\n",
    "        out = self.data.gpu() if self.device == 'gpu' else self.data\n",
    "        out = self.spectrogram(out)\n",
    "        return out\n",
    "\n",
    "    def iter_setup(self):\n",
    "        self.feed_input(self.data, self.batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExternalInputIterator(object):\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.files = glob.glob(p)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        self.n = len(self.files)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch = []\n",
    "        for _ in range(self.batch_size):\n",
    "            audio_name = self.files[self.i]\n",
    "            sr, wav = wavfile.read(audio_name)            \n",
    "            \n",
    "            batch.append([wav])\n",
    "            self.i = (self.i + 1) % self.n\n",
    "            print(f'batch shape = {wav.shape}')\n",
    "        return batch\n",
    "    \n",
    "class ExternalSourcePipeline(Pipeline):\n",
    "    def __init__(self, batch_size, num_threads, device_id):\n",
    "        super(ExternalSourcePipeline, self).__init__(batch_size,\n",
    "                                      num_threads,\n",
    "                                      device_id,\n",
    "                                      seed=12)\n",
    "\n",
    "        self.source = ops.ExternalSource(source = eii, num_outputs = 1)\n",
    "        print('pass')\n",
    "\n",
    "    def define_graph(self):\n",
    "        jpegs = self.source()\n",
    "        print('pass 2')\n",
    "        return jpegs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "eii = ExternalInputIterator(batch_size)\n",
    "iterator = iter(eii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n",
      "pass 2\n"
     ]
    }
   ],
   "source": [
    "pipe = ExternalSourcePipeline(batch_size=batch_size, num_threads=8, device_id = 0)\n",
    "pipe.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape = (19715328,)\n",
      "batch shape = (23058432,)\n",
      "batch shape = (21083904,)\n"
     ]
    }
   ],
   "source": [
    "dali_iter = DALIGenericIterator([pipe], ['audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'audio': tensor([[0., 0., 0.,  ..., 0., 0., 0.]])}]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Shapes do not match: DALI tensor has size [1, 23058432], but PyTorch Tensor has size [1, 19715328]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ae20e1042c95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdali_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nvidia/dali/plugin/pytorch.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m                     \u001b[0mfeed_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyt_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_stream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                     \u001b[0mfeed_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyt_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nvidia/dali/plugin/pytorch.py\u001b[0m in \u001b[0;36mfeed_ndarray\u001b[0;34m(dali_tensor, arr, cuda_stream)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mdali_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             (\"Shapes do not match: DALI tensor has size {0}\"\n\u001b[0;32m---> 59\u001b[0;31m             \", but PyTorch Tensor has size {1}\".format(dali_tensor.shape(), list(arr.size())))\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;31m#turn raw int to a c void pointer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mc_type_pointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Shapes do not match: DALI tensor has size [1, 23058432], but PyTorch Tensor has size [1, 19715328]"
     ]
    }
   ],
   "source": [
    "for i in dali_iter:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_cpu = pipe_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8941824,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_cpu.at(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
